{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bbox(bbox, frame, danger=False):\n",
    "    color = (220, 0, 0)\n",
    "    if danger:\n",
    "        color = (0, 0, 220)\n",
    "        frame = cv2.putText(frame, 'Inside drop zone!', (int(bbox[2]) + 10, int(bbox[3]) + 10), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                   1, color, 2, cv2.LINE_AA)\n",
    "    frame = cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[0]) + int(bbox[2]), int(bbox[1]) + int(bbox[3])),color, 2)\n",
    "    \n",
    "    return\n",
    "\n",
    "def draw_bboxes(bboxes, frame):\n",
    "    for bbox in bboxes:\n",
    "        draw_bbox(bbox, frame)\n",
    "        \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2 \n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"../inputs/frame_025000.PNG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = transform(image).to(device)\n",
    "image = image.unsqueeze(0)\n",
    "model.eval().to(device)\n",
    "\n",
    "detections = model(image)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boxes': tensor([[8.4481e+02, 5.3900e+02, 1.2800e+03, 7.1704e+02],\n",
       "         [3.9925e+02, 2.3516e+02, 7.6918e+02, 4.2749e+02],\n",
       "         [5.5654e+02, 3.1738e+02, 7.3751e+02, 4.2401e+02],\n",
       "         [3.5066e+02, 3.1899e+02, 7.3867e+02, 5.2937e+02],\n",
       "         [2.7420e+02, 6.4556e+02, 8.7616e+02, 7.2000e+02],\n",
       "         [2.6694e+01, 4.7367e+02, 1.8783e+02, 6.3951e+02],\n",
       "         [4.2836e+00, 7.8379e+01, 1.1514e+02, 2.5489e+02],\n",
       "         [3.9667e+02, 5.5048e+02, 8.3816e+02, 7.2000e+02],\n",
       "         [4.4070e+02, 2.7409e+02, 8.3417e+02, 4.9088e+02],\n",
       "         [8.8515e+02, 3.0139e+02, 9.9246e+02, 3.7204e+02],\n",
       "         [7.3724e+02, 2.5186e+02, 8.9528e+02, 3.4728e+02],\n",
       "         [5.9038e+02, 5.3367e+02, 1.0074e+03, 7.2000e+02],\n",
       "         [5.0885e+02, 2.6380e+02, 7.3180e+02, 3.9051e+02],\n",
       "         [7.8838e+02, 4.4069e+02, 9.3597e+02, 5.3356e+02],\n",
       "         [1.0768e+03, 4.8760e+02, 1.2751e+03, 7.1272e+02],\n",
       "         [3.9635e+02, 6.1906e+02, 5.0931e+02, 7.2000e+02],\n",
       "         [1.9439e+02, 3.4607e+02, 2.4952e+02, 4.2388e+02],\n",
       "         [1.0800e+03, 5.3482e+02, 1.1457e+03, 6.0927e+02],\n",
       "         [6.2521e+02, 5.4741e+02, 7.5672e+02, 6.3053e+02],\n",
       "         [6.7411e+02, 2.0071e+02, 9.0041e+02, 3.2629e+02],\n",
       "         [7.1793e+02, 1.9870e+02, 7.3370e+02, 2.2511e+02],\n",
       "         [9.4837e+02, 6.1310e+02, 1.1709e+03, 7.2000e+02],\n",
       "         [7.5502e+02, 2.8061e+02, 8.6385e+02, 3.4584e+02],\n",
       "         [1.4100e+02, 2.9491e+02, 2.6842e+02, 4.3647e+02],\n",
       "         [4.6889e+02, 6.6714e+02, 7.8724e+02, 7.2000e+02],\n",
       "         [6.2541e+02, 5.3832e+02, 7.9500e+02, 7.1290e+02],\n",
       "         [5.1299e+02, 6.1823e+02, 6.1860e+02, 7.1880e+02],\n",
       "         [5.8102e+02, 5.2233e+02, 7.4834e+02, 6.8909e+02],\n",
       "         [7.1209e+02, 1.9916e+02, 7.3940e+02, 2.3431e+02],\n",
       "         [1.1414e+03, 5.0052e+00, 1.2781e+03, 1.8913e+02],\n",
       "         [6.9961e+02, 2.0035e+02, 7.2648e+02, 2.3107e+02],\n",
       "         [5.9688e+02, 6.0515e+02, 7.5178e+02, 7.2000e+02],\n",
       "         [5.2817e+02, 5.9651e+02, 6.7726e+02, 7.2000e+02],\n",
       "         [6.8935e+02, 3.0812e+02, 7.8459e+02, 4.1190e+02],\n",
       "         [4.9275e+02, 1.6883e+01, 7.4465e+02, 4.8612e+02],\n",
       "         [4.0789e+02, 4.6615e+02, 1.1292e+03, 7.2000e+02],\n",
       "         [4.8004e+02, 3.7865e+02, 5.6180e+02, 4.4805e+02],\n",
       "         [1.1588e+03, 3.3909e+02, 1.2781e+03, 6.9893e+02],\n",
       "         [5.4646e+02, 1.9129e+02, 1.2800e+03, 6.1727e+02],\n",
       "         [4.1036e+02, 8.1310e+01, 8.3102e+02, 4.2717e+02],\n",
       "         [1.0302e+03, 3.0704e+00, 1.0670e+03, 4.3108e+01],\n",
       "         [1.9602e+02, 3.1082e+02, 2.3363e+02, 4.0545e+02],\n",
       "         [8.8907e+02, 2.7858e+02, 9.8426e+02, 3.4840e+02],\n",
       "         [8.5343e+02, 4.7077e+02, 9.0812e+02, 4.9254e+02],\n",
       "         [9.8356e+02, 3.3357e+02, 1.0590e+03, 3.7915e+02],\n",
       "         [8.3003e+02, 4.5318e+02, 9.3191e+02, 5.1854e+02],\n",
       "         [5.3667e+02, 4.0341e+02, 6.8325e+02, 5.3396e+02],\n",
       "         [8.6652e+02, 2.3708e+02, 9.4241e+02, 3.1401e+02],\n",
       "         [1.1044e+03, 2.9461e+02, 1.1771e+03, 3.4929e+02],\n",
       "         [3.6394e+02, 4.6879e+02, 6.6613e+02, 7.1925e+02],\n",
       "         [8.1646e+02, 4.3393e+02, 9.4095e+02, 5.0268e+02],\n",
       "         [8.9251e+02, 3.2853e+02, 9.6744e+02, 3.8071e+02],\n",
       "         [4.8066e+02, 3.0911e+02, 6.4577e+02, 4.4965e+02],\n",
       "         [1.0952e+03, 4.3809e+02, 1.1610e+03, 5.0102e+02],\n",
       "         [1.8108e+01, 3.7763e+02, 8.5626e+02, 7.2000e+02],\n",
       "         [0.0000e+00, 5.7374e+02, 1.2261e+02, 7.1237e+02],\n",
       "         [5.5873e+02, 4.4573e+02, 7.0330e+02, 5.7653e+02],\n",
       "         [5.2859e+02, 5.8776e+02, 5.7715e+02, 6.4600e+02],\n",
       "         [6.2293e+02, 3.7607e+02, 1.2800e+03, 7.2000e+02],\n",
       "         [0.0000e+00, 3.9608e+02, 4.3839e+02, 6.2321e+02],\n",
       "         [4.8665e+00, 2.0341e+01, 1.7193e+02, 2.7223e+02],\n",
       "         [1.0497e+03, 6.3720e+01, 1.0872e+03, 1.0460e+02],\n",
       "         [5.6601e+02, 2.7642e+02, 1.0816e+03, 4.9140e+02],\n",
       "         [1.1134e+03, 3.8188e+02, 1.2280e+03, 4.4051e+02],\n",
       "         [7.9802e+02, 4.4929e+02, 9.0441e+02, 5.1049e+02],\n",
       "         [1.0650e+03, 4.6323e+02, 1.1512e+03, 6.9625e+02],\n",
       "         [5.2205e+01, 4.5211e+02, 1.8449e+02, 5.8433e+02],\n",
       "         [1.1722e+03, 1.6161e+02, 1.2238e+03, 2.4039e+02],\n",
       "         [1.1152e+03, 2.9545e+02, 1.1518e+03, 3.4025e+02],\n",
       "         [6.4907e-01, 1.6222e+02, 1.1807e+02, 3.4318e+02],\n",
       "         [3.9831e+02, 4.9578e+02, 4.5371e+02, 5.5727e+02],\n",
       "         [2.8901e+01, 5.7295e+02, 4.2117e+02, 7.2000e+02],\n",
       "         [5.2953e+02, 6.7600e+02, 5.5754e+02, 7.0415e+02],\n",
       "         [1.0571e+00, 2.4111e+01, 5.0612e+01, 8.1430e+01],\n",
       "         [5.1218e+02, 2.5130e+02, 6.3980e+02, 3.7758e+02],\n",
       "         [0.0000e+00, 2.0827e+02, 3.5868e+02, 5.3835e+02],\n",
       "         [9.7865e+02, 3.9652e+02, 1.2547e+03, 6.7940e+02],\n",
       "         [9.8215e+02, 2.4565e+02, 1.0443e+03, 3.0871e+02],\n",
       "         [7.1004e+02, 1.9729e+02, 7.2611e+02, 2.2515e+02],\n",
       "         [5.7921e+02, 2.4870e+02, 6.8544e+02, 4.2287e+02],\n",
       "         [4.0821e+02, 4.0759e+02, 4.2970e+02, 4.5013e+02],\n",
       "         [1.2000e+03, 4.9317e+02, 1.2800e+03, 7.0471e+02],\n",
       "         [5.5920e+01, 4.3316e+02, 1.3855e+02, 5.1212e+02],\n",
       "         [0.0000e+00, 2.8347e+01, 8.9377e+01, 1.9280e+02],\n",
       "         [1.1043e+03, 5.0887e+02, 1.1510e+03, 6.0540e+02],\n",
       "         [2.9557e+02, 4.4959e+02, 8.8640e+02, 6.9003e+02],\n",
       "         [3.6903e+02, 4.2733e+02, 4.2516e+02, 4.8304e+02],\n",
       "         [3.3968e+02, 4.2960e+02, 4.6414e+02, 5.7088e+02],\n",
       "         [8.5539e+02, 2.8839e+02, 1.0037e+03, 4.4140e+02],\n",
       "         [6.7990e+02, 6.3701e+02, 8.2813e+02, 7.2000e+02],\n",
       "         [1.0904e+03, 3.0970e+02, 1.1797e+03, 3.6966e+02],\n",
       "         [2.6000e+02, 6.2925e+02, 4.4513e+02, 7.1962e+02],\n",
       "         [5.2487e+02, 2.2055e+02, 7.5361e+02, 5.4543e+02],\n",
       "         [4.2620e+02, 3.3012e+02, 5.7128e+02, 4.5636e+02],\n",
       "         [6.3949e+02, 2.8411e+02, 7.7744e+02, 4.2771e+02],\n",
       "         [9.3555e+02, 3.2209e+01, 1.1288e+03, 1.2534e+02],\n",
       "         [1.1693e+03, 5.9929e+02, 1.2777e+03, 7.1069e+02],\n",
       "         [5.2907e+02, 5.6632e+02, 5.9852e+02, 6.4824e+02],\n",
       "         [1.0145e+03, 3.2542e+02, 1.0468e+03, 3.5380e+02],\n",
       "         [9.2850e+02, 3.0191e+02, 1.0034e+03, 3.8300e+02]],\n",
       "        grad_fn=<StackBackward>),\n",
       " 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1]),\n",
       " 'scores': tensor([0.7677, 0.7520, 0.7375, 0.7314, 0.7276, 0.7217, 0.7176, 0.7151, 0.7136,\n",
       "         0.7135, 0.7089, 0.7065, 0.7051, 0.7050, 0.7018, 0.7017, 0.6979, 0.6967,\n",
       "         0.6923, 0.6906, 0.6901, 0.6900, 0.6894, 0.6892, 0.6890, 0.6883, 0.6875,\n",
       "         0.6838, 0.6830, 0.6825, 0.6786, 0.6763, 0.6751, 0.6739, 0.6733, 0.6710,\n",
       "         0.6706, 0.6688, 0.6686, 0.6682, 0.6673, 0.6669, 0.6656, 0.6644, 0.6642,\n",
       "         0.6640, 0.6617, 0.6598, 0.6598, 0.6595, 0.6593, 0.6565, 0.6546, 0.6539,\n",
       "         0.6538, 0.6521, 0.6510, 0.6506, 0.6503, 0.6501, 0.6500, 0.6496, 0.6491,\n",
       "         0.6491, 0.6490, 0.6452, 0.6436, 0.6418, 0.6418, 0.6415, 0.6410, 0.6405,\n",
       "         0.6398, 0.6393, 0.6386, 0.6386, 0.6385, 0.6384, 0.6380, 0.6376, 0.6370,\n",
       "         0.6368, 0.6360, 0.6344, 0.6341, 0.6328, 0.6325, 0.6319, 0.6298, 0.6293,\n",
       "         0.6286, 0.6265, 0.6240, 0.6227, 0.6227, 0.6222, 0.6203, 0.6201, 0.6191,\n",
       "         0.6185], grad_fn=<IndexBackward>)}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"../inputs/frame_025000.PNG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.cvtColor(np.asarray(image), cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(720, 1280, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = detections[\"boxes\"]\n",
    "labels = detections[\"labels\"]\n",
    "scores = detections[\"scores\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# person_indices = np.where(labels==1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_indices = [list(set(np.where(labels==1)[0]) & set(np.where(scores>0.5)[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
       "       68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,\n",
       "       85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_boxes = boxes[person_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, box in enumerate(person_boxes):\n",
    "    cv2.rectangle(\n",
    "        image,\n",
    "        (int(box[0]), int(box[1])),\n",
    "        (int(box[2]), int(box[3])),\n",
    "        (0,255,0), 2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, box in enumerate(person_boxes):\n",
    "#     cv2.rectangle(\n",
    "#         image,\n",
    "#         (int(box[0]), int(box[1])),\n",
    "#         (int(box[2]), int(box[3])),\n",
    "#         (0,255,0), 2\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite(\"image.jpg\", image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7677, 0.7520, 0.7375, 0.7314, 0.7276, 0.7217, 0.7176, 0.7151, 0.7136,\n",
       "        0.7135, 0.7089, 0.7065, 0.7051, 0.7050, 0.7018, 0.7017, 0.6979, 0.6967,\n",
       "        0.6923, 0.6906, 0.6901, 0.6900, 0.6894, 0.6892, 0.6890, 0.6883, 0.6875,\n",
       "        0.6838, 0.6830, 0.6825, 0.6786, 0.6763, 0.6751, 0.6739, 0.6733, 0.6710,\n",
       "        0.6706, 0.6688, 0.6686, 0.6682, 0.6673, 0.6669, 0.6656, 0.6644, 0.6642,\n",
       "        0.6640, 0.6617, 0.6598, 0.6598, 0.6595, 0.6593, 0.6565, 0.6546, 0.6539,\n",
       "        0.6538, 0.6521, 0.6510, 0.6506, 0.6503, 0.6501, 0.6500, 0.6496, 0.6491,\n",
       "        0.6491, 0.6490, 0.6452, 0.6436, 0.6418, 0.6418, 0.6415, 0.6410, 0.6405,\n",
       "        0.6398, 0.6393, 0.6386, 0.6386, 0.6385, 0.6384, 0.6380, 0.6376, 0.6370,\n",
       "        0.6368, 0.6360, 0.6344, 0.6341, 0.6328, 0.6325, 0.6319, 0.6298, 0.6293,\n",
       "        0.6286, 0.6265, 0.6240, 0.6227, 0.6227, 0.6222, 0.6203, 0.6201, 0.6191,\n",
       "        0.6185], grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[np.where(labels==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_boxes = boxes[np.where(labels==1)]\n",
    "person_scores = scores[np.where(labels==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7677, 0.7520, 0.7375, 0.7314, 0.7276, 0.7217, 0.7176, 0.7151, 0.7136,\n",
       "        0.7135, 0.7089, 0.7065, 0.7051, 0.7050, 0.7018, 0.7017, 0.6979, 0.6967,\n",
       "        0.6923, 0.6906, 0.6901, 0.6900, 0.6894, 0.6892, 0.6890, 0.6883, 0.6875,\n",
       "        0.6838, 0.6830, 0.6825, 0.6786, 0.6763, 0.6751, 0.6739, 0.6733, 0.6710,\n",
       "        0.6706, 0.6688, 0.6686, 0.6682, 0.6673, 0.6669, 0.6656, 0.6644, 0.6642,\n",
       "        0.6640, 0.6617, 0.6598, 0.6598, 0.6595, 0.6593, 0.6565, 0.6546, 0.6539,\n",
       "        0.6538, 0.6521, 0.6510, 0.6506, 0.6503, 0.6501, 0.6500, 0.6496, 0.6491,\n",
       "        0.6491, 0.6490, 0.6452, 0.6436, 0.6418, 0.6418, 0.6415, 0.6410, 0.6405,\n",
       "        0.6398, 0.6393, 0.6386, 0.6386, 0.6385, 0.6384, 0.6380, 0.6376, 0.6370,\n",
       "        0.6368, 0.6360, 0.6344, 0.6341, 0.6328, 0.6325, 0.6319, 0.6298, 0.6293,\n",
       "        0.6286, 0.6265, 0.6240, 0.6227, 0.6227, 0.6222, 0.6203, 0.6201, 0.6191,\n",
       "        0.6185], grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, box in enumerate(person_boxes):\n",
    "    cv2.rectangle(\n",
    "        image,\n",
    "        (int(box[0]), int(box[1])),\n",
    "        (int(box[2]), int(box[3])),\n",
    "        (0,255,0), 2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(720, 1280, 3)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite(\"image.jpg\", image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = draw_bboxes(person_boxes, np.asarray(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24343/1064015908.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvrms_env",
   "language": "python",
   "name": "cvrms_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
